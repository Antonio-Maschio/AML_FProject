{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1.7151896953582764 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import precision_score, confusion_matrix, accuracy_score, log_loss\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hyperopt import hp, tpe, fmin, Trials, space_eval\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Read the training data\n",
    "Data_train = pd.read_csv('Segmented_Nuclei_train.csv')\n",
    "\n",
    "# Extract features and target variable\n",
    "X = Data_train.drop(['Category'], axis=1)\n",
    "y = Data_train['Category']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(objective='binary:logistic', learning_rate=0.1,\n",
    "                            max_depth=8, eval_metric='logloss', n_estimators=300,\n",
    "                            seed=42, use_label_encoder=False, n_jobs = -1)\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['extent', 'mean_intensity', 'area_percentage', 'solidity',\n",
      "       'area_filled', 'area', 'area_bbox', 'euler_number'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "###Selecting the 20 most important features###\n",
    "\n",
    "importances = xgb_clf.feature_importances_\n",
    "sorted_indices = importances.argsort()[::-1]\n",
    "top_features = X.columns[sorted_indices]\n",
    "\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:18<00:00, 13.82s/trial, best loss: 0.22952606027694808]\n",
      "Best Hyperparameters: {'colsample_bytree': 0.9370615852680544, 'gamma': 0.729249267441817, 'learning_rate': 0.03584245887917516, 'max_depth': 10, 'min_child_weight': 1.4123423054280826, 'n_estimators': 800, 'subsample': 0.6485582507030402}\n",
      "Accuracy: 0.8948962945700303\n",
      "Confusion matrix: [[1996  185]\n",
      " [ 266 1844]]\n",
      "LogLoss: 0.2588742310719193\n",
      "Elapsed time: 5.081980466842651 seconds\n"
     ]
    }
   ],
   "source": [
    "###Hyperparameter tuning with Bayes search and cross-validation\n",
    "# Define the search space\n",
    "# X_20 = X[top_20_features]\n",
    "\n",
    "space = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 11, dtype=int)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'gamma': hp.uniform('gamma', 0, 1),\n",
    "    'n_estimators': hp.choice('n_estimators', np.arange(100, 1000, 100, dtype=int)),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0, 10)\n",
    "}\n",
    "\n",
    "# Objective function to minimize (log loss)\n",
    "def objective(params):\n",
    "    model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', seed=42, use_label_encoder=False, **params, n_jobs = -1)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_log_loss')\n",
    "    return -np.mean(scores)\n",
    "\n",
    "# Hyperparameter optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = space_eval(space, best)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', seed=42, use_label_encoder=False, **best_params, n_jobs = -1)\n",
    "start_time = time.time()\n",
    "best_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_proba.round())\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_proba.round())\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion matrix:\", conf_matrix)\n",
    "print(\"LogLoss:\", logloss)\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean LogLoss from Cross-Validation: 0.25712625542608397\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Define the best model with the best hyperparameters\n",
    "best_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', seed=42, use_label_encoder=False, **best_params, n_jobs=-1)\n",
    "\n",
    "# Define KFold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Arrays to store predictions and true labels\n",
    "val_logloss_scores = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Train the model on the training set\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    # Predict probabilities on the validation set\n",
    "    val_logloss = log_loss(y_test, y_pred_proba)\n",
    "    val_logloss_scores.append(val_logloss)\n",
    "\n",
    "mean_logloss = np.mean(val_logloss_scores)\n",
    "print(\"Mean LogLoss from Cross-Validation:\", mean_logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "\n",
    "# # Create a folder named 'solutions' if it doesn't exist\n",
    "# folder_name = 'solutions'\n",
    "# if not os.path.exists(folder_name):\n",
    "#     os.makedirs(folder_name)\n",
    "\n",
    "# Write = True\n",
    "# if Write:\n",
    "# # Your list of variables\n",
    "#     top_20_features_list = top_20_features.tolist()\n",
    "#     variables = top_20_features\n",
    "\n",
    "#     # Path to the CSV file\n",
    "    \n",
    "#     csv_file_path = os.path.join(folder_name, 'Classification_GeorgiosSevastakis_XGBoost1_VariableList.csv')\n",
    "\n",
    "#     # Open the CSV file in write mode\n",
    "#     with open(csv_file_path, mode='w', newline='') as file:\n",
    "#         # Create a CSV writer object\n",
    "#         writer = csv.writer(file)\n",
    "\n",
    "#         # Write each variable as a row in the CSV file\n",
    "#         for variable in variables:\n",
    "#             writer.writerow([variable])\n",
    "    \n",
    "#     data = y_pred_prob\n",
    "\n",
    "#     # Path to the CSV file\n",
    "#     csv_file_path = os.path.join(folder_name, 'Classification_GeorgiosSevastakis_XGBoost1.csv')\n",
    "    \n",
    "#     # Open the CSV file in write mode\n",
    "#     with open(csv_file_path, mode='w', newline='') as file:\n",
    "#         # Create a CSV writer object\n",
    "#         writer = csv.writer(file)\n",
    "        \n",
    "#         # Enumerate through the data and write each item along with its index as a row in the CSV file\n",
    "#         for index, item in enumerate(data, start=0):\n",
    "#             writer.writerow([index, item])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
